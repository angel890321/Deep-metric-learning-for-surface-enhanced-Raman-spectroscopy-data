{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637ab6e7",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8984c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92288fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fn = \"./data/combine_astii_astiii_filter_all_smoothing_norm.npy\"\n",
    "y_fn = \"./data/y_astii_astiii_filter_all_new.npy\"\n",
    "patient_fn = \"./data/patient_astii_astiii_filter_all_new.npy\"\n",
    "X_train_raw = np.load(X_fn,allow_pickle=True)\n",
    "y_train_raw = np.load(y_fn,allow_pickle=True)\n",
    "patient_train_raw = np.load(patient_fn,allow_pickle=True)\n",
    "\n",
    "classnames=['Acinetobacter', 'Enterobacter','Enterococcus','Escherichia','Klebsiella','Staphylococcus','Streptococcus','Salmonella','Pseudomonas','CoNS']\n",
    "\n",
    "df_p = pd.DataFrame(X_train_raw)\n",
    "df_p['Classes'] = y_train_raw\n",
    "df_p['Patient_ID'] = patient_train_raw \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1db3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(X_train_raw, np.ndarray):\n",
    "    X_train_raw = np.array(X_train_raw, dtype=np.float32)\n",
    "elif X_train_raw.dtype != np.float32:\n",
    "    X_train_raw = X_train_raw.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152edc6",
   "metadata": {},
   "source": [
    "### Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5425f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from model import Variant_LeNet,ResNet,RamanNet,ViT\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from losses import FocalLoss\n",
    "from losses import AdaptiveProxyAnchorLoss\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    auc,\n",
    "    roc_curve,\n",
    ")\n",
    "from pytorchtools import EarlyStopping\n",
    "import torch.optim as optim\n",
    "# Plot\n",
    "from plot import plot_10_genus_ROC_curve, plot_heatmap,plot_tsne_interactive_html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from config import ORDER, STRAINS\n",
    "import math\n",
    "\n",
    "# Training setting\n",
    "n_classes = 10\n",
    "epochs = 300 \n",
    "batch_size = 256\n",
    "num=50\n",
    "\n",
    "\n",
    "C_total = np.zeros((n_classes, n_classes))\n",
    "C = np.zeros((n_classes, n_classes))\n",
    "C_new = np.zeros((n_classes, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae18e81",
   "metadata": {},
   "source": [
    "## LeNet + CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets_spectrum import spectral_dataloader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from solver import Solver\n",
    "best_test_accuracy = 0\n",
    "low_test_accuracy = 1\n",
    "\n",
    "# metrics\n",
    "lenet_train_avg_accuracy = []\n",
    "lenet_avg_accuracy = []\n",
    "lenet_avg_roc = []\n",
    "\n",
    "selected_patient_combinations = set()\n",
    "    \n",
    "for i in range(num):\n",
    "    # Initialize empty DataFrames for this iteration's train and test sets\n",
    "    train_data = pd.DataFrame(columns=df_p.columns)\n",
    "    test_data = pd.DataFrame(columns=df_p.columns)\n",
    "\n",
    "    # 當前迭代的訓練集和測試集病患\n",
    "    max_attempts = 300 # 最大嘗試次數\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "            \n",
    "        test_patients = []\n",
    "        train_patients = []\n",
    "        \n",
    "        for class_label, group in df_p.groupby('Classes'):\n",
    "                # Select one random patient for the test set and the rest for the train set\n",
    "            num_patients = group['Patient_ID'].nunique()  # Number of unique patients in this class\n",
    "            test_patient_count = 1\n",
    "                \n",
    "            selected_test_patients = group['Patient_ID'].sample(n=test_patient_count, random_state=i).unique()\n",
    "            test_patients.extend(selected_test_patients)\n",
    "                \n",
    "            selected_train_patients = set(group['Patient_ID'].unique()) - set(selected_test_patients)\n",
    "            train_patients.extend(selected_train_patients)\n",
    "\n",
    "        patient_combination = (tuple(sorted(train_patients)), tuple(sorted(test_patients)))\n",
    "                    \n",
    "\n",
    "        # 檢查組合是否已存在\n",
    "        if patient_combination not in selected_patient_combinations:\n",
    "                \n",
    "            selected_patient_combinations.add(patient_combination)\n",
    "\n",
    "            test_data = df_p[df_p['Patient_ID'].isin(test_patients)]\n",
    "            train_data = df_p[df_p['Patient_ID'].isin(train_patients)]\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "\n",
    "    X_train, X_test = train_data.iloc[:, :-2].values , test_data.iloc[:, :-2].values\n",
    "    y_train, y_test = train_data['Classes'], test_data['Classes']\n",
    "    patient_train,patient_test  = train_data['Patient_ID'],test_data['Patient_ID']\n",
    "\n",
    "    y_train = y_train.values.astype(int)\n",
    "    y_test = y_test.values.astype(int)\n",
    "\n",
    "    dl_tr = spectral_dataloader(\n",
    "                            X_train, y_train,patient_train, idxs=None, batch_size=batch_size, shuffle=True\n",
    "                        )\n",
    "    dl_test = spectral_dataloader(X_test, y_test,patient_test,idxs=None, batch_size=batch_size, shuffle=False)\n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "    dataloaders = {\"train\": dl_tr, \"test\": dl_test}\n",
    "\n",
    "    model = Variant_LeNet(in_channels=1, out_channels=n_classes)\n",
    "\n",
    "    model_path = f\"best_variant_lenet_model_{i}.pth\"\n",
    "\n",
    "    \n",
    "    labels = np.array(y_train)  # y_train是所有訓練樣本的標籤\n",
    "\n",
    "    n_samples = len(labels)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    class_sample_count = np.array([(labels == t).sum() for t in range(n_classes)])\n",
    "\n",
    "    class_weights = n_samples / (n_classes * class_sample_count)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
    "    alpha = class_weights\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=alpha).cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "\n",
    "    solver = Solver(\n",
    "                i,dataloaders, model, model_path, 'cuda', n_classes, criterion,criterion_proxy=None,optimizer=optimizer,is_ce=True,gpu=-1)\n",
    "    print(i + 1)\n",
    "    solver.train(epochs)\n",
    "    C, y_true, y_pred, test_accuracy , y_pred_prob,train_acc,train_targets,targets,train_combined,combined ,train_patient_ids, test_patient_ids= solver.test()\n",
    "    C_total += C  # 將每次迭代的 C 加總到 C_total\n",
    "    lenet_train_avg_accuracy.append(train_acc)\n",
    "    lenet_avg_accuracy.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "\n",
    "        best_test_accuracy = test_accuracy\n",
    "            \n",
    "        plot_tsne_interactive_html(f\"variant_lenet_best_test_accuracy_combined_train\",f\"variant_lenet_best_test_accuracy_combined_test\",train_combined,combined,train_targets,targets, train_patient_ids, test_patient_ids,classnames )\n",
    "      \n",
    "        plot_heatmap(f\"variant_lenet_best_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    if test_accuracy < low_test_accuracy:\n",
    "\n",
    "        low_test_accuracy = test_accuracy\n",
    "        \n",
    "        plot_tsne_interactive_html(f\"variant_lenet_low_test_accuracy_combined_train\",f\"variant_lenet_low_test_accuracy_combined_test\",train_combined,combined,train_targets,targets,train_patient_ids, test_patient_ids, classnames )\n",
    "            \n",
    "        plot_heatmap(f\"variant_lenet_low_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "            v\n",
    "            for v in roc_auc.values()\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    lenet_avg_roc.append(auc_score)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        C_new[i] = np.round((C[i] / (counts[i] * num)), 2)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * C_new / C_new.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # calculate comfusion matrix\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # metrices result\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                    recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                    precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"F1 Score\": [\n",
    "                    f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    plot_10_genus_ROC_curve(f\"variant_lenet_{i}_roc_curve\", y_true, y_test, y_pred_prob)\n",
    "\n",
    "    plot_heatmap(f\"variant_lenet_{i}_heatmap\", cm,class_names=classnames)\n",
    "\n",
    "plot_heatmap(f\"variant_lenet_average_heatmap\", C_total,class_names=classnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f161583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max test acc: 0.961038961038961\n",
      "min test acc: 0.6434010152284264\n",
      "train mean: 0.9621270004314307\n",
      "train std: 0.05507424950676291\n",
      "mean: 0.8267466759374205\n",
      "std: 0.08459222857308561\n",
      "auc mean: 0.9718449085112113\n",
      "auc std: 0.027906651739174972\n"
     ]
    }
   ],
   "source": [
    "print(\"max test acc:\", np.max(lenet_avg_accuracy))\n",
    "print(\"min test acc:\", np.min(lenet_avg_accuracy))\n",
    "\n",
    "print(\"train mean:\", np.mean(lenet_train_avg_accuracy))\n",
    "print(\"train std:\", np.std(lenet_train_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"mean:\", np.mean(lenet_avg_accuracy))\n",
    "print(\"std:\", np.std(lenet_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"auc mean:\", np.mean(lenet_avg_roc))\n",
    "print(\"auc std:\", np.std(lenet_avg_roc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b422db",
   "metadata": {},
   "source": [
    "## ResNet + CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9465b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets_spectrum import spectral_dataloader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "best_test_accuracy = 0\n",
    "low_test_accuracy = 1\n",
    "\n",
    "resnet_train_avg_accuracy = []\n",
    "resnet_avg_accuracy = []\n",
    "resnet_avg_roc = []\n",
    "\n",
    "\n",
    "selected_patient_combinations = set()\n",
    "    \n",
    "for i in range(num):\n",
    "    # Initialize empty DataFrames for this iteration's train and test sets\n",
    "    train_data = pd.DataFrame(columns=df_p.columns)\n",
    "    test_data = pd.DataFrame(columns=df_p.columns)\n",
    "\n",
    "    # 當前迭代的訓練集和測試集病患\n",
    "    max_attempts = 300 # 最大嘗試次數\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "            \n",
    "        test_patients = []\n",
    "        train_patients = []\n",
    "\n",
    "\n",
    "        \n",
    "        for class_label, group in df_p.groupby('Classes'):\n",
    "                # Select one random patient for the test set and the rest for the train set\n",
    "            num_patients = group['Patient_ID'].nunique()  # Number of unique patients in this class\n",
    "            test_patient_count = 1\n",
    "                \n",
    "            selected_test_patients = group['Patient_ID'].sample(n=test_patient_count, random_state=i).unique()\n",
    "            test_patients.extend(selected_test_patients)\n",
    "                \n",
    "            selected_train_patients = set(group['Patient_ID'].unique()) - set(selected_test_patients)\n",
    "            train_patients.extend(selected_train_patients)\n",
    "\n",
    "        patient_combination = (tuple(sorted(train_patients)), tuple(sorted(test_patients)))\n",
    "                    \n",
    "\n",
    "        # 檢查組合是否已存在\n",
    "        if patient_combination not in selected_patient_combinations:\n",
    "                \n",
    "            selected_patient_combinations.add(patient_combination)\n",
    "\n",
    "            test_data = df_p[df_p['Patient_ID'].isin(test_patients)]\n",
    "            train_data = df_p[df_p['Patient_ID'].isin(train_patients)]\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "\n",
    "    X_train, X_test = train_data.iloc[:, :-2].values , test_data.iloc[:, :-2].values\n",
    "    y_train, y_test = train_data['Classes'], test_data['Classes']\n",
    "    patient_train,patient_test  = train_data['Patient_ID'],test_data['Patient_ID']\n",
    "\n",
    "    y_train = y_train.values.astype(int)\n",
    "    y_test = y_test.values.astype(int)\n",
    "\n",
    "    dl_tr = spectral_dataloader(\n",
    "                        X_train, y_train,patient_train, idxs=None, batch_size=batch_size, shuffle=True\n",
    "                    )\n",
    "    dl_test = spectral_dataloader(X_test, y_test,patient_test,idxs=None, batch_size=batch_size, shuffle=False)\n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "    dataloaders = {\"train\": dl_tr, \"test\": dl_test}\n",
    "\n",
    "    # setting resnet\n",
    "    layers = 6\n",
    "    hidden_size = 100\n",
    "    block_size = 2\n",
    "    hidden_sizes = [hidden_size] * layers\n",
    "    num_blocks = [block_size] * layers\n",
    "    input_dim = 400\n",
    "    model = ResNet(\n",
    "                    hidden_sizes,\n",
    "                    num_blocks,\n",
    "                    input_dim=input_dim,\n",
    "                    in_channels=64,\n",
    "                    n_classes=10,\n",
    "                )\n",
    "\n",
    "    model_path = f\"best_resnet_model_{i}.pth\"\n",
    "\n",
    "    \n",
    "    labels = np.array(y_train)  # y_train是所有訓練樣本的標籤\n",
    "\n",
    "    n_samples = len(labels)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    class_sample_count = np.array([(labels == t).sum() for t in range(n_classes)])\n",
    "\n",
    "    class_weights = n_samples / (n_classes * class_sample_count)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
    "    alpha = class_weights\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=alpha).cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "\n",
    "    solver = Solver(\n",
    "                i,dataloaders, model, model_path, 'cuda', n_classes, criterion,criterion_proxy=None,optimizer=optimizer,is_ce=True,gpu=-1)\n",
    "\n",
    "    print(i + 1)\n",
    "    solver.train(epochs)\n",
    "    C, y_true, y_pred, test_accuracy , y_pred_prob,train_acc,train_targets,targets,train_combined,combined ,train_patient_ids, test_patient_ids= solver.test()\n",
    "    C_total += C  # 將每次迭代的 C 加總到 C_total\n",
    "    resnet_train_avg_accuracy.append(train_acc)\n",
    "    resnet_avg_accuracy.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "\n",
    "        best_test_accuracy = test_accuracy\n",
    "            \n",
    "        plot_tsne_interactive_html(f\"resnet_best_test_accuracy_combined_train\",f\"resnet_best_test_accuracy_combined_test\",train_combined,combined,train_targets,targets, train_patient_ids, test_patient_ids,classnames )\n",
    "      \n",
    "        plot_heatmap(f\"resnet_best_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    if test_accuracy < low_test_accuracy:\n",
    "\n",
    "        low_test_accuracy = test_accuracy\n",
    "        \n",
    "        plot_tsne_interactive_html(f\"resnet_low_test_accuracy_combined_train\",f\"resnet_low_test_accuracy_combined_test\",train_combined,combined,train_targets,targets,train_patient_ids, test_patient_ids, classnames )\n",
    "            \n",
    "        plot_heatmap(f\"resnet_low_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "            v\n",
    "            for v in roc_auc.values()\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    resnet_avg_roc.append(auc_score)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        C_new[i] = np.round((C[i] / (counts[i] * num)), 2)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * C_new / C_new.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # calculate comfusion matrix\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # metrices result\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                    recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                    precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"F1 Score\": [\n",
    "                    f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_10_genus_ROC_curve(f\"resnet_{i}_roc_curve\", y_true, y_test, y_pred_prob)\n",
    "\n",
    "    plot_heatmap(f\"resnet_{i}_heatmap\", cm,class_names=classnames)\n",
    "\n",
    "plot_heatmap(f\"resnet_average_heatmap\", C_total,class_names=classnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e687a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max test acc: 0.9544419134396356\n",
      "min test acc: 0.66125\n",
      "train mean: 0.9520946940839164\n",
      "train std: 0.07090336091886695\n",
      "mean: 0.8141751212614554\n",
      "std: 0.07467371675711117\n",
      "auc mean: 0.9727106450937576\n",
      "auc std: 0.024929068514520165\n"
     ]
    }
   ],
   "source": [
    "print(\"max test acc:\", np.max(resnet_avg_accuracy))\n",
    "print(\"min test acc:\", np.min(resnet_avg_accuracy))\n",
    "\n",
    "print(\"train mean:\", np.mean(resnet_train_avg_accuracy))\n",
    "print(\"train std:\", np.std(resnet_train_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"mean:\", np.mean(resnet_avg_accuracy))\n",
    "print(\"std:\", np.std(resnet_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"auc mean:\", np.mean(resnet_avg_roc))\n",
    "print(\"auc std:\", np.std(resnet_avg_roc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d38ca5",
   "metadata": {},
   "source": [
    "## RamanNet + CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets_sliding_window import sliding_spectral_dataloader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "best_test_accuracy = 0\n",
    "low_test_accuracy = 1\n",
    "\n",
    "ramannet_train_avg_accuracy = []\n",
    "ramannet_avg_accuracy = []\n",
    "ramannet_avg_roc = []\n",
    "\n",
    "\n",
    "selected_patient_combinations = set()\n",
    "    \n",
    "for i in range(num):\n",
    "    # Initialize empty DataFrames for this iteration's train and test sets\n",
    "    train_data = pd.DataFrame(columns=df_p.columns)\n",
    "    test_data = pd.DataFrame(columns=df_p.columns)\n",
    "\n",
    "    # 當前迭代的訓練集和測試集病患\n",
    "    max_attempts = 300 # 最大嘗試次數\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "            \n",
    "        test_patients = []\n",
    "        train_patients = []\n",
    "\n",
    "        if i == 23:\n",
    "            i =50\n",
    "\n",
    "        if i == 35:\n",
    "            i =51\n",
    "\n",
    "        \n",
    "        for class_label, group in df_p.groupby('Classes'):\n",
    "                # Select one random patient for the test set and the rest for the train set\n",
    "            num_patients = group['Patient_ID'].nunique()  # Number of unique patients in this class\n",
    "            test_patient_count = 1\n",
    "                \n",
    "            selected_test_patients = group['Patient_ID'].sample(n=test_patient_count, random_state=i).unique()\n",
    "            test_patients.extend(selected_test_patients)\n",
    "                \n",
    "            selected_train_patients = set(group['Patient_ID'].unique()) - set(selected_test_patients)\n",
    "            train_patients.extend(selected_train_patients)\n",
    "\n",
    "        patient_combination = (tuple(sorted(train_patients)), tuple(sorted(test_patients)))\n",
    "                    \n",
    "\n",
    "        # 檢查組合是否已存在\n",
    "        if patient_combination not in selected_patient_combinations:\n",
    "                \n",
    "            selected_patient_combinations.add(patient_combination)\n",
    "\n",
    "            test_data = df_p[df_p['Patient_ID'].isin(test_patients)]\n",
    "            train_data = df_p[df_p['Patient_ID'].isin(train_patients)]\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "\n",
    "    X_train, X_test = train_data.iloc[:, :-2].values , test_data.iloc[:, :-2].values\n",
    "    y_train, y_test = train_data['Classes'], test_data['Classes']\n",
    "    patient_train,patient_test  = train_data['Patient_ID'],test_data['Patient_ID']\n",
    "\n",
    "    y_train = y_train.values.astype(int)\n",
    "    y_test = y_test.values.astype(int)\n",
    "\n",
    "    dl_tr = sliding_spectral_dataloader(\n",
    "                        X_train, y_train,patient_train, idxs=None, batch_size=batch_size, shuffle=True\n",
    "                    )\n",
    "    dl_test = sliding_spectral_dataloader(X_test, y_test,patient_test,idxs=None, batch_size=batch_size, shuffle=False)\n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "    dataloaders = {\"train\": dl_tr, \"test\": dl_test}\n",
    "\n",
    "    model = RamanNet(w_len=50, n_windows=14,n_classes=10)\n",
    "\n",
    "    model_path = f\"best_ramannet_model_{i}.pth\"\n",
    "\n",
    "    \n",
    "    labels = np.array(y_train)  # y_train是所有訓練樣本的標籤\n",
    "\n",
    "    n_samples = len(labels)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    class_sample_count = np.array([(labels == t).sum() for t in range(n_classes)])\n",
    "\n",
    "    class_weights = n_samples / (n_classes * class_sample_count)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
    "    alpha = class_weights\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=alpha).cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "\n",
    "    solver = Solver(\n",
    "                i,dataloaders, model, model_path, 'cuda', n_classes, criterion,criterion_proxy=None,optimizer=optimizer,is_ce=True,gpu=-1)\n",
    "\n",
    "    print(i + 1)\n",
    "    solver.train(epochs)\n",
    "    C, y_true, y_pred, test_accuracy , y_pred_prob,train_acc,train_targets,targets,train_combined,combined ,train_patient_ids, test_patient_ids= solver.test()\n",
    "    C_total += C  # 將每次迭代的 C 加總到 C_total\n",
    "    ramannet_train_avg_accuracy.append(train_acc)\n",
    "    ramannet_avg_accuracy.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "\n",
    "        best_test_accuracy = test_accuracy\n",
    "            \n",
    "        plot_tsne_interactive_html(f\"ramannet_best_test_accuracy_combined_train\",f\"ramannet_best_test_accuracy_combined_test\",train_combined,combined,train_targets,targets, train_patient_ids, test_patient_ids,classnames )\n",
    "      \n",
    "        plot_heatmap(f\"ramannet_best_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    if test_accuracy < low_test_accuracy:\n",
    "\n",
    "        low_test_accuracy = test_accuracy\n",
    "        \n",
    "        plot_tsne_interactive_html(f\"ramannet_low_test_accuracy_combined_train\",f\"ramannet_low_test_accuracy_combined_test\",train_combined,combined,train_targets,targets,train_patient_ids, test_patient_ids, classnames )\n",
    "            \n",
    "        plot_heatmap(f\"ramannet_low_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "            v\n",
    "            for v in roc_auc.values()\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    ramannet_avg_roc.append(auc_score)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        C_new[i] = np.round((C[i] / (counts[i] * num)), 2)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * C_new / C_new.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # calculate comfusion matrix\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # metrices result\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                    recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                    precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"F1 Score\": [\n",
    "                    f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_10_genus_ROC_curve(f\"ramannet_{i}_roc_curve\", y_true, y_test, y_pred_prob)\n",
    "\n",
    "    plot_heatmap(f\"ramannet_{i}_heatmap\", cm,class_names=classnames)\n",
    "\n",
    "plot_heatmap(f\"ramannet_average_heatmap\", C_total,class_names=classnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d263009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max test acc: 0.9521640091116174\n",
      "min test acc: 0.5906976744186047\n",
      "train mean: 0.897457576430947\n",
      "train std: 0.05443504421692891\n",
      "mean: 0.8254445328623597\n",
      "std: 0.08520033310924602\n",
      "auc mean: 0.9775121954794647\n",
      "auc std: 0.021915895443454406\n"
     ]
    }
   ],
   "source": [
    "print(\"max test acc:\", np.max(ramannet_avg_accuracy))\n",
    "print(\"min test acc:\", np.min(ramannet_avg_accuracy))\n",
    "\n",
    "print(\"train mean:\", np.mean(ramannet_train_avg_accuracy))\n",
    "print(\"train std:\", np.std(ramannet_train_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"mean:\", np.mean(ramannet_avg_accuracy))\n",
    "print(\"std:\", np.std(ramannet_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"auc mean:\", np.mean(ramannet_avg_roc))\n",
    "print(\"auc std:\", np.std(ramannet_avg_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a50969",
   "metadata": {},
   "source": [
    "## ViT+CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11136e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "class Solver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        i,\n",
    "        dataloaders,\n",
    "        model,\n",
    "        model_path,\n",
    "        device,\n",
    "        n_classes,\n",
    "        alpha,\n",
    "        gpu=-1,\n",
    "    ):\n",
    "        self.i = i\n",
    "        self.dataloaders = dataloaders\n",
    "        self.device = device\n",
    "        self.net = model\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.n_classes = n_classes\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=alpha).cuda()\n",
    "        self.get_num_labeled_class = 2\n",
    "        self.optimizer = optim.Adam(self.net.parameters(),lr = 0.001)\n",
    "        self.model_path = model_path\n",
    "\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        if phase == \"train\":\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "            \n",
    "        dataloader = self.dataloaders[phase]\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "            \n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = Variable(inputs).to(self.device), Variable(targets.long()).to(self.device)\n",
    "            out,emb= self.net(inputs)\n",
    "            \n",
    "            loss = self.criterion(out, targets)\n",
    "            \n",
    "            outputs = torch.argmax(out.detach(), dim=1)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += targets.size(0)\n",
    "            correct += outputs.eq(targets).cpu().sum().item()\n",
    "\n",
    "        total_loss /= (batch_idx + 1)\n",
    "        total_acc = correct / total\n",
    "\n",
    "                \n",
    "        #print(\"\\nepoch %d: %s average loss: %.3f | acc: %.2f%% (%d/%d)\"\n",
    "                #% (epoch + 1, phase, total_loss, 100.0 * total_acc, correct, total))\n",
    "\n",
    "        return total_loss,total_acc,out,targets\n",
    "            \n",
    "\n",
    "    def train(self, epochs):\n",
    "        best_loss = float(\"inf\")\n",
    "        best_acc = 0\n",
    "        epoch_breaks_classifer = 0\n",
    "        train_losses = []\n",
    "        train_acces = [] \n",
    "        test_losses = []\n",
    "        test_acces = [] \n",
    "\n",
    "\n",
    "        early_stopping_classifier = EarlyStopping(patience=25, mode='acc', verbose=True)\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            train_loss,train_acc,out,targets =self.iterate(epoch, \"train\")\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_acces.append(train_acc)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                test_loss,test_acc,out,targets= self.iterate(epoch, \"test\")\n",
    "                    \n",
    "                test_losses.append(test_loss)\n",
    "                test_acces.append(test_acc)\n",
    "\n",
    "                if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    checkpoint = {\n",
    "                        \"epoch\": epoch,\n",
    "                         \"train_acc\":train_acc,\n",
    "                        \"test_acc\":test_acc,\n",
    "                        \"test_loss\": test_loss,\n",
    "                        \"state_dict\": self.net.state_dict(),\n",
    "                    }\n",
    "                    #print(\"best test acc found\")\n",
    "                    torch.save(checkpoint, self.model_path)\n",
    "\n",
    "                early_stopping_classifier(test_acc)\n",
    "\n",
    "                epoch_breaks_classifer+= 1\n",
    "\n",
    "                if early_stopping_classifier.early_stop:\n",
    "                    break  # 停止訓練\n",
    "        \n",
    "        if not early_stopping_classifier.early_stop:\n",
    "            epoch_breaks_classifer = epochs\n",
    "        \n",
    "\n",
    "    def test_iterate(self, phase):\n",
    "        self.net.eval()\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_pred_prob = []\n",
    "        \n",
    "        features_out = []\n",
    "        features_combine = []\n",
    "        targets_combine = []\n",
    "        patient_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "                inputs, targets = Variable(inputs).to('cuda'), Variable(targets.long()).to('cuda')\n",
    "                out,emb = self.net(inputs)\n",
    "                outputs = torch.argmax(out.detach(), dim=1)\n",
    "                outputs_prob = nn.functional.softmax(out.detach(), dim=1)\n",
    "                y_pred.append(outputs.cpu().numpy())\n",
    "                y_pred_prob.append(outputs_prob.cpu().numpy())\n",
    "            \n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                \n",
    "                targets_combine.append(targets.detach().cpu().numpy())\n",
    "                features_out.append(out.cpu().numpy())\n",
    "                features_combine.append(emb.detach().cpu().numpy())\n",
    "\n",
    "            targets_combine = np.concatenate(targets_combine, axis=0) \n",
    "            features_out = np.concatenate(features_out, axis=0)\n",
    "            features_combine = np.concatenate(features_combine, axis=0)\n",
    "        \n",
    "        return (\n",
    "                np.hstack(y_pred),\n",
    "                np.hstack(y_true),\n",
    "                np.vstack(y_pred_prob),\n",
    "                features_out,\n",
    "                features_combine,\n",
    "                targets_combine,\n",
    "            )\n",
    "    def test(self):\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        train_acc = checkpoint[\"train_acc\"]\n",
    "        test_acc = checkpoint[\"test_acc\"]\n",
    "        self.net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        #print(\"load model at epoch {}, with test acc : {:.3f}\".format(epoch+1, test_acc))\n",
    "\n",
    "        \n",
    "        _, _ ,_,train_out,train_combined,train_targets = self.test_iterate(\"train\")\n",
    "        y_pred, y_true ,y_pred_prob,out,combined,targets= self.test_iterate(\"test\")\n",
    "        \n",
    "\n",
    "        return (\n",
    "            confusion_matrix(y_true, y_pred),\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            accuracy_score(y_true, y_pred),\n",
    "            y_pred_prob,\n",
    "            train_acc,\n",
    "            train_targets,\n",
    "            targets,\n",
    "            train_combined,\n",
    "            combined,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffee644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.utils.data as Data\n",
    "from plot import plot_tsne\n",
    "best_test_accuracy = 0\n",
    "low_test_accuracy = 1\n",
    "\n",
    "ViT_train_avg_accuracy = []\n",
    "ViT_avg_accuracy = []\n",
    "ViT_avg_roc = []\n",
    "\n",
    "\n",
    "selected_patient_combinations = set()\n",
    "    \n",
    "for i in range(num):\n",
    "\n",
    "    if i == 23:\n",
    "        i =50\n",
    "\n",
    "    if i == 35:\n",
    "        i =51\n",
    "\n",
    "    if i == 38:\n",
    "        i =52\n",
    "    train_data = pd.DataFrame(columns=df_p.columns)\n",
    "    test_data = pd.DataFrame(columns=df_p.columns)\n",
    "\n",
    "         # 當前迭代的訓練集和測試集病患\n",
    "    max_attempts = 100  # 最大嘗試次數\n",
    "    combination_found = False  # 記錄是否找到新組合\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        test_patients = []\n",
    "        train_patients = []\n",
    "\n",
    "        for class_label, group in df_p.groupby('Classes'):\n",
    "                # Select one random patient for the test set and the rest for the train set\n",
    "            num_patients = group['Patient_ID'].nunique()  # Number of unique patients in this class\n",
    "            test_patient_count = 1\n",
    "                \n",
    "            selected_test_patients = group['Patient_ID'].sample(n=test_patient_count, random_state=i).unique()\n",
    "            test_patients.extend(selected_test_patients)\n",
    "                \n",
    "            selected_train_patients = set(group['Patient_ID'].unique()) - set(selected_test_patients)\n",
    "            train_patients.extend(selected_train_patients)\n",
    "\n",
    "        patient_combination = (tuple(sorted(train_patients)), tuple(sorted(test_patients)))\n",
    "\n",
    "        # 檢查組合是否已存在\n",
    "        if patient_combination not in selected_patient_combinations:\n",
    "            selected_patient_combinations.add(patient_combination)\n",
    "\n",
    "            test_data = df_p[df_p['Patient_ID'].isin(test_patients)]\n",
    "            train_data = df_p[df_p['Patient_ID'].isin(train_patients)]\n",
    "\n",
    "            combination_found = True\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_train, X_test = train_data.iloc[:, :-2].values , test_data.iloc[:, :-2].values\n",
    "    y_train, y_test = train_data['Classes'], test_data['Classes']\n",
    "\n",
    "    y_train = y_train.values.astype(int)\n",
    "    y_test = y_test.values.astype(int)\n",
    "    \n",
    "    # Set up dataloaders\n",
    "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "    y_train = torch.from_numpy(y_train.astype(np.longlong))\n",
    "\n",
    "\n",
    "    if isinstance(X_test, np.ndarray):\n",
    "        X_test = torch.from_numpy(X_test).float()\n",
    "    else:\n",
    "        X_test = X_test.float()  # 如果 X_test 已經是張量\n",
    "\n",
    "    if isinstance(y_test, np.ndarray):\n",
    "        y_test = torch.from_numpy(y_test).long()\n",
    "    else:\n",
    "        y_test = y_test.long()  # 如果 y_test 已經是張量\n",
    "\n",
    "\n",
    "    dataset_tr = Data.TensorDataset(X_train, y_train)\n",
    "    dl_tr = Data.DataLoader(dataset_tr, batch_size=batch_size, shuffle = True)\n",
    "        \n",
    "        \n",
    "    dataset_test = Data.TensorDataset(X_test, y_test)\n",
    "    dl_test = Data.DataLoader(dataset_test, batch_size=batch_size, shuffle = False)\n",
    "    \n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "\n",
    "    dataloaders = {\"train\": dl_tr, \"test\": dl_test}\n",
    "\n",
    "    model = ViT(image_size = (1,400), patch_size = (1, 16), num_classes = n_classes, channels = 1, dim = 64, depth = 3, heads = 6, mlp_dim = 128, dropout = 0.1, emb_dropout = 0.1)\n",
    "\n",
    "    model_path = f\"best_ViT_{i}.pth\"\n",
    "\n",
    "    \n",
    "    labels = np.array(y_train)  # y_train是所有訓練樣本的標籤\n",
    "\n",
    "    n_samples = len(labels)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    class_sample_count = np.array([(labels == t).sum() for t in range(n_classes)])\n",
    "\n",
    "    class_weights = n_samples / (n_classes * class_sample_count)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
    "    alpha = class_weights\n",
    "    solver = Solver(\n",
    "                i,dataloaders, model, model_path, 'cuda', n_classes, alpha\n",
    "            )\n",
    "    print(i + 1)\n",
    "    solver.train(epochs)\n",
    "    C, y_true, y_pred, test_accuracy , y_pred_prob,train_acc,train_targets,targets,train_combined,combined = solver.test()\n",
    "    C_total += C  # 將每次迭代的 C 加總到 C_total\n",
    "    ViT_train_avg_accuracy.append(train_acc)\n",
    "    ViT_avg_accuracy.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "\n",
    "        best_test_accuracy = test_accuracy\n",
    "            \n",
    "        plot_tsne(f\"ViT_best_test_accuracy_combined_train\",f\"ViT_best_test_accuracy_combined_test\",train_combined,combined,train_targets,targets,classnames )\n",
    "      \n",
    "        plot_heatmap(f\"ViT_best_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    if test_accuracy < low_test_accuracy:\n",
    "\n",
    "        low_test_accuracy = test_accuracy\n",
    "        \n",
    "        plot_tsne(f\"ViT_low_test_accuracy_combined_train\",f\"ViT_low_test_accuracy_combined_test\",train_combined,combined,train_targets,targets, classnames )\n",
    "            \n",
    "        plot_heatmap(f\"ViT_low_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "            v\n",
    "            for v in roc_auc.values()\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    ViT_avg_roc.append(auc_score)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        C_new[i] = np.round((C[i] / (counts[i] * num)), 2)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * C_new / C_new.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # calculate comfusion matrix\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # metrices result\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                    recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                    precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"F1 Score\": [\n",
    "                    f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_10_genus_ROC_curve(f\"ViT_{i}_roc_curve\", y_true, y_test, y_pred_prob)\n",
    "\n",
    "    plot_heatmap(f\"ViT_{i}_heatmap\", cm,class_names=classnames)\n",
    "\n",
    "plot_heatmap(f\"ViT_average_heatmap\", C_total,class_names=classnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f532b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max test acc: 0.958997722095672\n",
      "min test acc: 0.6677927927927928\n",
      "train mean: 0.8848680165541407\n",
      "train std: 0.0596626751928599\n",
      "mean: 0.8139190678409861\n",
      "std: 0.07135639584838985\n",
      "auc mean: 0.9764489140411229\n",
      "auc std: 0.01662871029969768\n"
     ]
    }
   ],
   "source": [
    "print(\"max test acc:\", np.max(ViT_avg_accuracy))\n",
    "print(\"min test acc:\", np.min(ViT_avg_accuracy))\n",
    "\n",
    "print(\"train mean:\", np.mean(ViT_train_avg_accuracy))\n",
    "print(\"train std:\", np.std(ViT_train_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"mean:\", np.mean(ViT_avg_accuracy))\n",
    "print(\"std:\", np.std(ViT_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"auc mean:\", np.mean(ViT_avg_roc))\n",
    "print(\"auc std:\", np.std(ViT_avg_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6a69e",
   "metadata": {},
   "source": [
    "## LeNet + MAPA + Focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b00585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets_spectrum import spectral_dataloader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from losses import MultiAdaptiveProxyAnchorLoss\n",
    "from solver import  Solver\n",
    "best_test_accuracy = 0\n",
    "low_test_accuracy = 1\n",
    "\n",
    "# metrics\n",
    "lenet_mapa_focal_train_avg_accuracy = []\n",
    "lenet_mapa_focal_avg_accuracy = []\n",
    "lenet_mapa_focal_avg_roc = []\n",
    "\n",
    "selected_patient_combinations = set()\n",
    "    \n",
    "for i in range(num):\n",
    "    # Initialize empty DataFrames for this iteration's train and test sets\n",
    "    train_data = pd.DataFrame(columns=df_p.columns)\n",
    "    test_data = pd.DataFrame(columns=df_p.columns)\n",
    "\n",
    "    # 當前迭代的訓練集和測試集病患\n",
    "    max_attempts = 300 # 最大嘗試次數\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "            \n",
    "        test_patients = []\n",
    "        train_patients = []\n",
    "\n",
    "        if i == 23:\n",
    "            i =50\n",
    "\n",
    "        if i == 35:\n",
    "            i =51\n",
    "\n",
    "        \n",
    "        for class_label, group in df_p.groupby('Classes'):\n",
    "                # Select one random patient for the test set and the rest for the train set\n",
    "            num_patients = group['Patient_ID'].nunique()  # Number of unique patients in this class\n",
    "            test_patient_count = 1\n",
    "                \n",
    "            selected_test_patients = group['Patient_ID'].sample(n=test_patient_count, random_state=i).unique()\n",
    "            test_patients.extend(selected_test_patients)\n",
    "                \n",
    "            selected_train_patients = set(group['Patient_ID'].unique()) - set(selected_test_patients)\n",
    "            train_patients.extend(selected_train_patients)\n",
    "\n",
    "        patient_combination = (tuple(sorted(train_patients)), tuple(sorted(test_patients)))\n",
    "                    \n",
    "\n",
    "        # 檢查組合是否已存在\n",
    "        if patient_combination not in selected_patient_combinations:\n",
    "                \n",
    "            selected_patient_combinations.add(patient_combination)\n",
    "\n",
    "            test_data = df_p[df_p['Patient_ID'].isin(test_patients)]\n",
    "            train_data = df_p[df_p['Patient_ID'].isin(train_patients)]\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "\n",
    "    X_train, X_test = train_data.iloc[:, :-2].values , test_data.iloc[:, :-2].values\n",
    "    y_train, y_test = train_data['Classes'], test_data['Classes']\n",
    "    patient_train,patient_test  = train_data['Patient_ID'],test_data['Patient_ID']\n",
    "\n",
    "    y_train = y_train.values.astype(int)\n",
    "    y_test = y_test.values.astype(int)\n",
    "\n",
    "    dl_tr = spectral_dataloader(\n",
    "                        X_train, y_train,patient_train, idxs=None, batch_size=batch_size, shuffle=True\n",
    "                    )\n",
    "    dl_test = spectral_dataloader(X_test, y_test,patient_test,idxs=None, batch_size=batch_size, shuffle=False)\n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "    dataloaders = {\"train\": dl_tr, \"test\": dl_test}\n",
    "\n",
    "    model = Variant_LeNet(in_channels=1, out_channels=n_classes)\n",
    "\n",
    "    model_path = f\"best_variant_lenet_mapa_focal_model_{i}.pth\"\n",
    "\n",
    "    class_counts = np.bincount(y_train)\n",
    "    num_classes = len(class_counts)\n",
    "    total_samples = len(y_train)\n",
    "\n",
    "    class_weights = []\n",
    "    for count in class_counts:\n",
    "        weight = 1 / (count / total_samples)\n",
    "        class_weights.append(weight)\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "    alpha = class_weights\n",
    "    \n",
    "    criterion = FocalLoss(alpha=alpha, gamma=2).cuda()\n",
    "\n",
    "    criterion_proxy =MultiAdaptiveProxyAnchorLoss(nb_classes=n_classes, sz_embed=256, mrg=0.5, alpha=32,\\\n",
    "                                             nb_proxies=2).cuda()\n",
    "\n",
    "    param_groups = [\n",
    "            {'params': list(set(model.parameters()).difference(set(model.embedding.parameters())))},\n",
    "            {'params': model.embedding.parameters() , 'lr':float(0.001) * 1},\n",
    "            {'params': criterion_proxy.mrg,'lr':float(0.001) },\n",
    "            {'params': criterion_proxy.proxies, 'lr':float(0.001) * 10}\n",
    "        ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=0.001, weight_decay = 0.001)\n",
    "\n",
    "    solver = Solver(\n",
    "                i,dataloaders, model, model_path, 'cuda', n_classes, criterion,criterion_proxy,optimizer,is_ce=False,gpu=-1)\n",
    "\n",
    "    print(i + 1)\n",
    "    solver.train(epochs)\n",
    "    C, y_true, y_pred, test_accuracy , y_pred_prob,train_acc,train_targets,targets,train_combined,combined ,train_patient_ids, test_patient_ids= solver.test()\n",
    "    C_total += C  # 將每次迭代的 C 加總到 C_total\n",
    "    lenet_mapa_focal_train_avg_accuracy.append(train_acc)\n",
    "    lenet_mapa_focal_avg_accuracy.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "\n",
    "        best_test_accuracy = test_accuracy\n",
    "            \n",
    "        plot_tsne_interactive_html(f\"lenet_mapa_focal_best_test_accuracy_combined_train\",f\"lenet_mapa_focal_best_test_accuracy_combined_test\",train_combined,combined,train_targets,targets, train_patient_ids, test_patient_ids,classnames )\n",
    "      \n",
    "        plot_heatmap(f\"lenet_mapa_focal_best_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    if test_accuracy < low_test_accuracy:\n",
    "\n",
    "        low_test_accuracy = test_accuracy\n",
    "        \n",
    "        plot_tsne_interactive_html(f\"lenet_mapa_focal_low_test_accuracy_combined_train\",f\"lenet_mapa_focal_low_test_accuracy_combined_test\",train_combined,combined,train_targets,targets,train_patient_ids, test_patient_ids, classnames )\n",
    "            \n",
    "        plot_heatmap(f\"lenet_mapa_focal_low_test_accuracy_heatmap\", C,class_names=classnames)\n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "            v\n",
    "            for v in roc_auc.values()\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    lenet_mapa_focal_avg_roc.append(auc_score)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        C_new[i] = np.round((C[i] / (counts[i] * num)), 2)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * C_new / C_new.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # calculate comfusion matrix\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # metrices result\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                    recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                    precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            \"F1 Score\": [\n",
    "                    f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "                ],\n",
    "            }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_10_genus_ROC_curve(f\"lenet_mapa_focal_{i}_roc_curve\", y_true, y_test, y_pred_prob)\n",
    "\n",
    "    plot_heatmap(f\"lenet_mapa_focal_{i}_heatmap\", cm,class_names=classnames)\n",
    "\n",
    "plot_heatmap(f\"lenet_mapa_focal_average_heatmap\", C_total,class_names=classnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29041d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max test acc: 0.9551208285385501\n",
      "min test acc: 0.6598984771573604\n",
      "train mean: 0.9586720593714128\n",
      "train std: 0.06265306599347507\n",
      "mean: 0.8232951914205411\n",
      "std: 0.08143043258807192\n",
      "auc mean: 0.9745946331558163\n",
      "auc std: 0.026876053904732713\n"
     ]
    }
   ],
   "source": [
    "print(\"max test acc:\", np.max(lenet_mapa_focal_avg_accuracy))\n",
    "print(\"min test acc:\", np.min(lenet_mapa_focal_avg_accuracy))\n",
    "\n",
    "print(\"train mean:\", np.mean(lenet_mapa_focal_train_avg_accuracy))\n",
    "print(\"train std:\", np.std(lenet_mapa_focal_train_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"mean:\", np.mean(lenet_mapa_focal_avg_accuracy))\n",
    "print(\"std:\", np.std(lenet_mapa_focal_avg_accuracy))\n",
    "\n",
    "    \n",
    "print(\"auc mean:\", np.mean(lenet_mapa_focal_avg_roc))\n",
    "print(\"auc std:\", np.std(lenet_mapa_focal_avg_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1053ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "Model                         Mean Accuracy  Std Dev   \n",
      "-------------------------------------------------------\n",
      " LeNet + MAPA + Focal         0.8233     0.0814\n",
      "RamanNet + CE                 0.8254     0.0852\n",
      "LeNet + CE                    0.8267     0.0846\n",
      "ResNet + CE                   0.8142     0.0747\n",
      "ViT + CE                      0.8139     0.0714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 將模型名稱與對應數據組成字典\n",
    "mean_accuracy = {\n",
    "    \" LeNet + MAPA + Focal\": 0.8232951914205411,\n",
    "    \"RamanNet + CE\": 0.8254445328623597,\n",
    "    \"LeNet + CE\": 0.8267466759374205,\n",
    "    \"ResNet + CE\": 0.8141751212614554,\n",
    "    \"ViT + CE\": 0.8139190678409861\n",
    "}\n",
    "\n",
    "std_accuracy = {\n",
    "    \" LeNet + MAPA + Focal\": 0.08143043258807192,\n",
    "    \"RamanNet + CE\":  0.08520033310924602,\n",
    "    \"LeNet + CE\": 0.08459222857308561,\n",
    "    \"ResNet + CE\": 0.07467371675711117,\n",
    "    \"ViT + CE\": 0.07135639584838985\n",
    "}\n",
    "\n",
    "# 輸出結果\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"{'Model':<30}{'Mean Accuracy':<15}{'Std Dev':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for name in mean_accuracy:\n",
    "    mean = mean_accuracy[name]\n",
    "    std = std_accuracy[name]\n",
    "    print(f\"{name:<30}{mean:.4f}{'':<5}{std:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
